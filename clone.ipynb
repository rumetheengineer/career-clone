{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a72affc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rumes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import smtplib\n",
    "from google import genai\n",
    "from google.genai.types import Tool, FunctionDeclaration, Schema, GenerateContentConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1ed1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3a5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(message, reason):\n",
    "    print(f\"Push: {message}\")\n",
    "    \n",
    "    # Load environment variables inside the function\n",
    "    push_mail = os.getenv(\"CLONE_EMAIL\")\n",
    "    push_password = os.getenv(\"CLONE_PASSWORD\")\n",
    "    \n",
    "    print(f\"Email: {push_mail}\")\n",
    "    print(f\"Password: {'*' * len(push_password) if push_password else 'None'}\")\n",
    "    \n",
    "    if not push_mail or not push_password:\n",
    "        print(\"Error: CLONE_EMAIL or CLONE_PASSWORD not found in environment variables\")\n",
    "        print(\"Please create a .env file with CLONE_EMAIL and CLONE_PASSWORD\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        mail=smtplib.SMTP(\"smtp.gmail.com\")\n",
    "        mail.starttls()\n",
    "        mail.login(user=push_mail, password=push_password)\n",
    "        mail.sendmail(\n",
    "            from_addr=push_mail, \n",
    "            to_addrs=\"rumesefia@gmail.com\", \n",
    "            msg=f\"Subject:{reason}\\n\\n{message}\"\n",
    "            )\n",
    "        mail.quit()\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending email: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd45120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_user_details(email, name=\"Name not provided\", notes=\"notes not provided\"):\n",
    "    push(f\"Recording interest from {name} with email {email} and notes {notes}\", \"Recording User Details\")\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e330d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_unknown_question(question):\n",
    "    push(f\"Recording {question} that I don't know the answer to\", \"Recording Unknown Question\")\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3c1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [\n",
    "    {\n",
    "      \"function_declarations\": [\n",
    "        {\n",
    "          \"name\": \"record_user_details\",\n",
    "          \"description\": \"Records user details including email, name, and notes. Used for capturing user interest or contact information.\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "              \"email\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"The user's email address (required).\"\n",
    "              },\n",
    "              \"name\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"The user's name (optional, defaults to 'Name not provided').\"\n",
    "              },\n",
    "              \"notes\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Any additional notes or details provided by the user (optional, defaults to 'notes not provided').\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"email\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"function_declarations\": [\n",
    "        {\n",
    "          \"name\": \"record_unknown_question\",\n",
    "          \"description\": \"Records a question that the Gemini agent does not know the answer to, for future reference or training.\",\n",
    "          \"parameters\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "              \"question\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"The question that the agent could not answer.\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6d24bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(tool_calls):\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "        tool = globals().get(tool_name)\n",
    "        result = tool(**arguments) if tool else {}\n",
    "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef030951",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"Profile.pdf\")\n",
    "\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba60028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_reader = PdfReader(\"resume.pdf\")\n",
    "\n",
    "resume = \"\"\n",
    "for page in resume_reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        resume += text\n",
    "\n",
    "with open(\"summary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    summary = file.read()\n",
    "\n",
    "name = \"Sefia Oghenerume\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adacb6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background, current professional resume, and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool.\"\n",
    "\n",
    "system_prompt += f\"Respond in a natural, conversational, and engaging tone. Avoid using bullet points, markdown formatting, or lists, \\\n",
    "    unless explicitly requested or required to drive home a point. Write as if you are having a friendly, semi-formal \\\n",
    "    chat with the userâ€”use contractions, vary your sentence structure, and keep your language warm and personable.\"\n",
    "\n",
    "system_prompt += f\"You are to represent the engineer of this agent and the author of this prompt {name} \\\n",
    "    prioritize the information in the summary although not openly divulge it to the user. \\\n",
    "    it is simply a guide to give context of the person you are deployed to represent. \\\n",
    "    Following the summary, prioritise information in the resume, followed by the github stated in the summary, then the linkedin last\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n## Resume:\\n{resume}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa23226",
   "metadata": {},
   "outputs": [],
   "source": [
    "client  =  genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4377dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=\"What is the capital of France?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b2ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "\n",
    "def chat(message, history):\n",
    "    config = GenerateContentConfig(tools=tools_list)\n",
    "    gemini_messages = []\n",
    "    if system_prompt:\n",
    "        gemini_messages.append({\"role\": \"user\", \"parts\": [{\"text\": system_prompt}]})\n",
    "    for h_msg in history:\n",
    "        if h_msg[\"role\"] == \"user\":\n",
    "            gemini_messages.append({\"role\": \"user\", \"parts\": [{\"text\": h_msg[\"content\"]}]})\n",
    "        elif h_msg[\"role\"] == \"assistant\":\n",
    "            parts = []\n",
    "            if \"content\" in h_msg and h_msg[\"content\"]:\n",
    "                parts.append({\"text\": h_msg[\"content\"]})\n",
    "            if \"tool_calls\" in h_msg and h_msg[\"tool_calls\"]:\n",
    "                for tc in h_msg[\"tool_calls\"]:\n",
    "                    parts.append({\"function_call\": {\"name\": tc.function.name, \"args\": json.loads(tc.function.arguments)}})\n",
    "            gemini_messages.append({\"role\": \"model\", \"parts\": parts})\n",
    "        elif h_msg[\"role\"] == \"tool\":\n",
    "            gemini_messages.append(h_msg)\n",
    "    gemini_messages.append({\"role\": \"user\", \"parts\": [{\"text\": message}]})\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=gemini_messages, config=config)\n",
    "        \n",
    "        tool_calls_from_gemini = []\n",
    "        candidates = getattr(response, \"candidates\", None)\n",
    "        if candidates and candidates[0].content:\n",
    "            parts = getattr(candidates[0].content, \"parts\", None)\n",
    "        tool_calls_from_gemini = [\n",
    "            part.function_call for part in parts if getattr(part, \"function_call\", None)\n",
    "        ]\n",
    "\n",
    "        if tool_calls_from_gemini and response.candidates[0].content.parts:\n",
    "            gemini_messages.append({\"role\": \"model\", \"parts\": response.candidates[0].content.parts})\n",
    "            results = handle_tool_calls(tool_calls_from_gemini)\n",
    "            gemini_messages.extend(results)\n",
    "        else:\n",
    "            done = True\n",
    "\n",
    "    final_text_parts = [part.text for part in response.candidates[0].content.parts if part.text]\n",
    "    return final_text_parts[0] if final_text_parts else \"\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d86b27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c737d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handle_tool_calls(tool_calls):\n",
    "#     results = []\n",
    "#     for tool_call in tool_calls:\n",
    "#         tool_name = tool_call.name\n",
    "#         arguments = tool_call.args\n",
    "#         print(f\"Tool called: {tool_name} with arguments: {arguments}\", flush=True)\n",
    "\n",
    "#         tool = globals().get(tool_name)\n",
    "#         result_data = tool(**arguments) if tool else {\"error\": f\"Tool '{tool_name}' not found.\"}\n",
    "\n",
    "#         results.append({\n",
    "#             \"role\": \"function\",\n",
    "#             \"parts\": [{\"function_response\": {\"name\": tool_name, \"response\": result_data}}]\n",
    "#         })\n",
    "#     return results\n",
    "\n",
    "    #GOOD ONE\n",
    "    #But can be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "518e9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat(message, history):\n",
    "#     messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=messages, config=config)\n",
    "#         finish_reason = response.choices[0].finish_reason\n",
    "#         if finish_reason==\"tool_calls\":\n",
    "#             message = response.choices[0].message\n",
    "#             tool_calls = message.tool_calls\n",
    "#             results = handle_tool_calls(tool_calls)\n",
    "#             messages.append(message)\n",
    "#             messages.extend(results)\n",
    "#         else:\n",
    "#             done = True\n",
    "#     return messages[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccf61d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat(message, history):\n",
    "#     gemini_messages = []\n",
    "#     if system_prompt:\n",
    "#         gemini_messages.append({\"role\": \"user\", \"parts\": [{\"text\": system_prompt}]})\n",
    "\n",
    "#     for h_msg in history:\n",
    "#         if h_msg[\"role\"] == \"user\":\n",
    "#             gemini_messages.append({\"role\": \"user\", \"parts\": [{\"text\": h_msg[\"content\"]}]})\n",
    "#         elif h_msg[\"role\"] == \"assistant\":\n",
    "#             parts = []\n",
    "#             if \"content\" in h_msg and h_msg[\"content\"]:\n",
    "#                 parts.append({\"text\": h_msg[\"content\"]})\n",
    "#             if \"tool_calls\" in h_msg and h_msg[\"tool_calls\"]:\n",
    "#                 for tc in h_msg[\"tool_calls\"]:\n",
    "#                     parts.append({\"function_call\": {\"name\": tc.function.name, \"args\": json.loads(tc.function.arguments)}})\n",
    "#             gemini_messages.append({\"role\": \"model\", \"parts\": parts})\n",
    "#         elif h_msg[\"role\"] == \"tool\":\n",
    "#             gemini_messages.append(h_msg)\n",
    "\n",
    "#     gemini_messages.append({\"role\": \"user\", \"parts\": [{\"text\": message}]})\n",
    "\n",
    "#     final_response_content = \"\"\n",
    "#     done = False\n",
    "\n",
    "#     while not done:\n",
    "#         try:\n",
    "#             response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=gemini_messages)\n",
    "\n",
    "#             if not response.candidates or not response.candidates[0].content:\n",
    "#                 print(\"ERROR: Gemini response is empty or malformed.\")\n",
    "#                 final_response_content = \"I encountered an issue getting a response from the AI.\"\n",
    "#                 done = True\n",
    "#                 break\n",
    "\n",
    "#             model_response_parts = response.candidates[0].content.parts\n",
    "#             if model_response_parts:\n",
    "#                 tool_calls_from_gemini = [\n",
    "#                     part.function_call for part in model_response_parts if hasattr(part, \"function_call\")\n",
    "#                 ]\n",
    "\n",
    "#                 if tool_calls_from_gemini:\n",
    "#                     gemini_messages.append({\"role\": \"model\", \"parts\": model_response_parts})\n",
    "#                     tool_execution_results = handle_tool_calls(tool_calls_from_gemini)\n",
    "#                     gemini_messages.extend(tool_execution_results)\n",
    "#                 else:\n",
    "#                     final_response_content = \"\".join([\n",
    "#                         part.text for part in model_response_parts if hasattr(part, \"text\") and part.text is not None\n",
    "#                     ])\n",
    "#                     done = True\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"ERROR: An unexpected error occurred during chat: {e}\")\n",
    "#             final_response_content = f\"I encountered an unexpected error: {e}\"\n",
    "#             done = True\n",
    "\n",
    "#     return final_response_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ecb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. Prepare tools and config\n",
    "# tools = [\n",
    "#     types.Tool(function_declarations=[\n",
    "#         record_unknown_question_json,\n",
    "#         record_user_details_json\n",
    "#     ])\n",
    "# ]\n",
    "# config = types.GenerateContentConfig(tools=tools)\n",
    "\n",
    "# # 2. The chat function\n",
    "# def chat(message, history):\n",
    "#     # Compose the conversation\n",
    "#     contents = [\n",
    "#         types.Content(role=\"user\", parts=[types.Part(text=message)])\n",
    "#     ]\n",
    "#     # Add history if needed (optional, for multi-turn)\n",
    "#     # for user_msg, bot_msg in history:\n",
    "#     #     contents.append(types.Content(role=\"user\", parts=[types.Part(text=user_msg)]))\n",
    "#     #     contents.append(types.Content(role=\"model\", parts=[types.Part(text=bot_msg)]))\n",
    "\n",
    "#     # 3. Call the model\n",
    "#     response = genai.Client().models.generate_content(\n",
    "#         model=\"gemini-2.5-flash\",  # or your preferred model\n",
    "#         contents=contents,\n",
    "#         config=config,\n",
    "#     # )\n",
    "\n",
    "    # 4. Check for function call\n",
    "    # part = response.candidates[0].content.parts[0]\n",
    "    # if hasattr(part, \"function_call\") and part.function_call:\n",
    "    #     fn = part.function_call\n",
    "    #     # Map function name to your Python function\n",
    "    #     fn_map = {\n",
    "    #         \"record_unknown_question\": record_unknown_question,\n",
    "    #         \"record_user_details\": user_details,\n",
    "    #     }\n",
    "    #     result = fn_map[fn.name](**fn.args)\n",
    "    #     # 5. Send function result back to model for final response\n",
    "    #     function_response_part = types.Part.from_function_response(\n",
    "    #         name=fn.name,\n",
    "    #         response={\"result\": result},\n",
    "    #     )\n",
    "    #     contents.append(response.candidates[0].content)  # model's function call message\n",
    "    #     contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
    "    #     final_response = genai.Client().models.generate_content(\n",
    "    #         model=\"gemini-2.5-flash\",\n",
    "    #         contents=contents,\n",
    "    #         config=config,\n",
    "    #     )\n",
    "    #     return final_response.text\n",
    "    # else:\n",
    "    #     # No function call, just return the text\n",
    "    #     return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad3e86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat(message, history):\n",
    "#     # Gemini needs the full conversation as a single string\n",
    "#     conversation = f\"System: {system_prompt}\\n\"\n",
    "#     for user_msg, bot_msg in history:\n",
    "#         conversation += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n",
    "#     conversation += f\"User: {message}\\nAssistant:\"\n",
    "\n",
    "#     model = genai.GenerativeModel(\"gemini-1.5-flash-latest\", tools=tools)\n",
    "\n",
    "#     # Generate response with tool support\n",
    "#     response = model.generate_content([conversation], tool_config={\"enable_tool_calling\": True})\n",
    "\n",
    "#     # If Gemini wants to call a tool, handle it\n",
    "#     if hasattr(response, \"candidates\") and response.candidates:\n",
    "#         candidate = response.candidates[0]\n",
    "#         if hasattr(candidate, \"content\") and candidate.content:\n",
    "#             # Check for tool calls in the response\n",
    "#             tool_calls = getattr(candidate, \"tool_calls\", None)\n",
    "#             if tool_calls:\n",
    "#                 # Run the tool(s) and append results to conversation\n",
    "#                 results = handle_tool_calls(tool_calls)\n",
    "#                 # Optionally, you can append tool results to the conversation and re-query Gemini for a final answer\n",
    "#                 # For simplicity, return the tool result or a message\n",
    "#                 return results[0][\"content\"] if results else \"Tool called, but no result.\"\n",
    "#             else:\n",
    "#                 return candidate.content.parts[0].text if candidate.content.parts else candidate.content.text\n",
    "#     # Fallback: return the plain response text\n",
    "#     return response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
